% This file was created with Citavi 6.15.2.0

@proceedings{.2010,
 year = {2010},
 title = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 address = {Madison, WI, USA},
 publisher = {Omnipress},
 isbn = {9781605589077},
 series = {ICML'10}
}


@article{Zheng.2018,
 abstract = {Recently telecom fraud has become a serious problem especially in developing countries such as China. At present, it can be very difficult to coordinate different agencies to prevent fraud completely. In this paper we study how to detect large transfers that are sent from victims deceived by fraudsters at the receiving bank. We propose a new generative adversarial network (GAN) based model to calculate for each large transfer a probability that it is fraudulent, such that the bank can take appropriate measures to prevent potential fraudsters to take the money if the probability exceeds a threshold. The inference model uses a deep denoising autoencoder to effectively learn the complex probabilistic relationship among the input features, and employs adversarial training that establishes a minimax game between a discriminator and a generator to accurately discriminate between positive samples and negative samples in the data distribution. We show that the model outperforms a set of well-known classification methods in experiments, and its applications in two commercial banks have reduced losses of about 10 million RMB in twelve weeks and significantly improved their business reputation.},
 author = {Zheng, Yu-Jun and Zhou, Xiao-Han and Sheng, Wei-Guo and Xue, Yu and Chen, Sheng-Yong},
 year = {2018},
 title = {Generative adversarial network based telecom fraud detection at the receiving bank},
 pages = {78--86},
 volume = {102},
 journal = {Neural networks : the official journal of the International Neural Network Society},
 doi = {10.1016/j.neunet.2018.02.015}
}


@article{Yi.2019,
 abstract = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
 author = {Yi, Xin and Walia, Ekta and Babyn, Paul},
 year = {2019},
 title = {Generative adversarial network in medical imaging: A review},
 pages = {101552},
 volume = {58},
 journal = {Medical image analysis},
 doi = {10.1016/j.media.2019.101552}
}


@article{Yamashita.2018,
 abstract = {Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care. KEY POINTS: • Convolutional neural network is a class of deep learning methods which has become dominant in various computer vision tasks and is attracting interest across a variety of domains, including radiology. • Convolutional neural network is composed of multiple building blocks, such as convolution layers, pooling layers, and fully connected layers, and is designed to automatically and adaptively learn spatial hierarchies of features through a backpropagation algorithm. • Familiarity with the concepts and advantages, as well as limitations, of convolutional neural network is essential to leverage its potential to improve radiologist performance and, eventually, patient care.},
 author = {Yamashita, Rikiya and Nishio, Mizuho and Do, Richard Kinh Gian and Togashi, Kaori},
 year = {2018},
 title = {Convolutional neural networks: an overview and application in radiology},
 pages = {611--629},
 volume = {9},
 number = {4},
 issn = {1869-4101},
 journal = {Insights into imaging},
 doi = {10.1007/s13244-018-0639-9}
}


@misc{Ulyanov.2016,
 abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture{\_}nets. Full paper can be found at arXiv:1701.02096.},
 author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
 date = {2016},
 title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1607.08022}
}


@misc{Srivastava.2017,
 abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.

Published as a conference paper at NIPS, 2017},
 author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
 date = {2017},
 title = {VEEGAN: Reducing Mode Collapse in GANs using Implicit Variational Learning},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1705.07761}
}


@article{Sharma.2017,
 author = {Sharma, Sagar and Sharma, Simone and Athaiya, Anidhya},
 year = {2017},
 title = {Activation functions in neural networks},
 pages = {310--316},
 volume = {6},
 number = {12},
 journal = {Towards Data Sci}
}


@article{Sharma.2022,
 author = {Sharma, Neha and Sharma, Reecha and Jindal, Neeru},
 year = {2022},
 title = {Comparative analysis of CycleGAN and AttentionGAN on face aging application},
 volume = {47},
 number = {1},
 issn = {0256-2499},
 journal = {Sādhanā},
 doi = {10.1007/s12046-022-01807-4}
}


@article{Saxena.2021,
 author = {Saxena, Divya and Cao, Jiannong},
 year = {2021},
 title = {Generative adversarial networks (GANs) challenges, solutions, and future directions},
 pages = {1--42},
 volume = {54},
 number = {3},
 journal = {ACM Computing Surveys (CSUR)}
}


@misc{Salimans.2016,
 abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3{\%}. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
 author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
 date = {2016},
 title = {Improved Techniques for Training GANs},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1606.03498}
}


@misc{Radford.2015,
 abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.

Under review as a conference paper at ICLR 2016},
 author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
 date = {2015},
 title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1511.06434}
}


@incollection{PhillipIsola.,
 abstract = {1. Introduction

Many problems in image processing, computer graphics,},
 author = {{Phillip Isola} and {Jun-Yan Zhu} and {Tinghui Zhou} and {Alexei A. Efros}},
 title = {Image-to-Image Translation with Conditional Adversarial Networks}
}


@article{Pan.2019,
 author = {Pan, Zhaoqing and Yu, Weijie and Yi, Xiaokai and Khan, Asifullah and Yuan, Feng and Zheng, Yuhui},
 year = {2019},
 title = {Recent Progress on Generative Adversarial Networks (GANs): A Survey},
 pages = {36322--36333},
 volume = {7},
 journal = {IEEE Access},
 doi = {10.1109/ACCESS.2019.2905015}
}


@misc{OShea.2015,
 abstract = {The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs.

This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.

10 pages, 5 figures},
 author = {O'Shea, Keiron and Nash, Ryan},
 date = {2015},
 title = {An Introduction to Convolutional Neural Networks},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1511.08458}
}


@inproceedings{Nair.2010,
 abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these {\textquotedbl}Stepped Sigmoid Units{\textquotedbl} are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
 author = {Nair, Vinod and Hinton, Geoffrey E.},
 title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
 pages = {807--814},
 publisher = {Omnipress},
 isbn = {9781605589077},
 series = {ICML'10},
 booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
 year = {2010},
 address = {Madison, WI, USA}
}


@article{Liu.2021,
 abstract = {Magnetic Resonance Imaging (MRI) guided Radiation Therapy is a hot topic in the current studies of radiotherapy planning, which requires using MRI to generate synthetic Computed Tomography (sCT). Despite recent progress in image-to-image translation, it remains challenging to apply such techniques to generate high-quality medical images. This paper proposes a novel framework named Multi-Cycle GAN, which uses the Pseudo-Cycle Consistent module to control the consistency of generation and the domain control module to provide additional identical constraints. Besides, we design a new generator named Z-Net to improve the accuracy of anatomy details. Extensive experiments show that Multi-Cycle GAN outperforms state-of-the-art CT synthesis methods such as Cycle GAN, which improves MAE to 0.0416, ME to 0.0340, PSNR to 39.1053.},
 author = {Liu, Yanxia and Chen, Anni and Shi, Hongyu and Huang, Sijuan and Zheng, Wanjia and Liu, Zhiqiang and Zhang, Qin and Yang, Xin},
 year = {2021},
 title = {CT synthesis from MRI using multi-cycle GAN for head-and-neck radiation therapy},
 pages = {101953},
 volume = {91},
 journal = {Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society},
 doi = {10.1016/j.compmedimag.2021.101953}
}


@misc{Liu.2022,
 abstract = {Generative Adversarial Networks (GANs) have shown compelling results in various tasks and applications in recent years. However, mode collapse remains a critical problem in GANs. In this paper, we propose a novel training pipeline to address the mode collapse issue of GANs. Different from existing methods, we propose to generalize the discriminator as feature embedding and maximize the entropy of distributions in the embedding space learned by the discriminator. Specifically, two regularization terms, i.e., Deep Local Linear Embedding (DLLE) and Deep Isometric feature Mapping (DIsoMap), are designed to encourage the discriminator to learn the structural information embedded in the data, such that the embedding space learned by the discriminator can be well-formed. Based on the well-learned embedding space supported by the discriminator, a non-parametric entropy estimator is designed to efficiently maximize the entropy of embedding vectors, playing as an approximation of maximizing the entropy of the generated distribution. By improving the discriminator and maximizing the distance of the most similar samples in the embedding space, our pipeline effectively reduces the mode collapse without sacrificing the quality of generated samples. Extensive experimental results show the effectiveness of our method, which outperforms the GAN baseline, MaF-GAN on CelebA (9.13 vs. 12.43 in FID) and surpasses the recent state-of-the-art energy-based model on the ANIME-FACE dataset (2.80 vs. 2.26 in Inception score). The code is available at https://github.com/HaozheLiu-ST/MEE

Accepted by AAAI'2023 (Oral); Code is released at https://github.com/HaozheLiu-ST/MEE},
 author = {Liu, Haozhe and Li, Bing and Wu, Haoqian and Liang, Hanbang and Huang, Yawen and Li, Yuexiang and Ghanem, Bernard and Zheng, Yefeng},
 date = {2022},
 title = {Combating Mode Collapse in GANs via Manifold Entropy Estimation},
 publisher = {arXiv},
 doi = {10.48550/arXiv.2208.12055}
}


@misc{Kingma.2014,
 abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.

Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
 author = {Kingma, Diederik P. and Ba, Jimmy},
 date = {2014},
 title = {Adam: A Method for Stochastic Optimization},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1412.6980}
}


@article{Jain.2020,
 author = {Jain, Deepak Kumar and Zareapoor, Masoumeh and Jain, Rachna and Kathuria, Abhishek and Bachhety, Shivam},
 year = {2020},
 title = {GAN-Poser: an improvised bidirectional GAN model for human motion prediction},
 pages = {14579--14591},
 volume = {32},
 number = {18},
 journal = {Neural Computing and Applications}
}


@misc{Huang.2017,
 abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color {\&} spatial controls, all using a single feed-forward neural network.

ICCV 2017. Code is available: https://github.com/xunhuang1995/AdaIN-style},
 author = {Huang, Xun and Belongie, Serge},
 date = {2017},
 title = {Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1703.06868}
}


@article{Hong.2020,
 author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
 year = {2020},
 title = {How Generative Adversarial Networks and Their Variants Work},
 pages = {1--43},
 volume = {52},
 number = {1},
 issn = {0360-0300},
 journal = {ACM Computing Surveys},
 doi = {10.1145/3301282}
}


@misc{He.2015,
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.

The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.

Tech report},
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 date = {2015},
 title = {Deep Residual Learning for Image Recognition},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1512.03385}
}


@article{HazemAbdelmotaalAhmedA.AbdouAhmedF.OmarDaliaMohamedElSebaityKhaledAbdelazeem.2021,
 author = {{Hazem Abdelmotaal, Ahmed A. Abdou, Ahmed F. Omar, Dalia Mohamed El-Sebaity, Khaled Abdelazeem}},
 year = {2021},
 title = {Pix2pix Conditional Generative Adversarial Networks forScheimpflug Camera Color-Coded Corneal TomographyImage Generation}
}


@misc{Eckerli.2021,
 abstract = {Modelling in finance is a challenging task: the data often has complex statistical properties and its inner workings are largely unknown. Deep learning algorithms are making progress in the field of data-driven modelling, but the lack of sufficient data to train these models is currently holding back several new applications. Generative Adversarial Networks (GANs) are a neural network architecture family that has achieved good results in image generation and is being successfully applied to generate time series and other types of financial data. The purpose of this study is to present an overview of how these GANs work, their capabilities and limitations in the current state of research with financial data, and present some practical applications in the industry. As a proof of concept, three known GAN architectures were tested on financial time series, and the generated data was evaluated on its statistical properties, yielding solid results. Finally, it was shown that GANs have made considerable progress in their finance applications and can be a solid additional tool for data scientists in this field.},
 author = {Eckerli, Florian and Osterrieder, Joerg},
 date = {2021},
 title = {Generative Adversarial Networks in finance: an overview},
 publisher = {arXiv},
 doi = {10.48550/arXiv.2106.06364}
}


@article{Creswell.2018,
 author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
 year = {2018},
 title = {Generative Adversarial Networks: An Overview},
 pages = {53--65},
 volume = {35},
 number = {1},
 issn = {1053-5888},
 journal = {IEEE Signal Processing Magazine},
 doi = {10.1109/MSP.2017.2765202}
}


@misc{Chu.2017,
 abstract = {CycleGAN (Zhu et al. 2017) is one recent successful approach to learn a transformation between two image distributions. In a series of experiments, we demonstrate an intriguing property of the model: CycleGAN learns to {\textquotedbl}hide{\textquotedbl} information about a source image into the images it generates in a nearly imperceptible, high-frequency signal. This trick ensures that the generator can recover the original sample and thus satisfy the cyclic consistency requirement, while the generated image remains realistic. We connect this phenomenon with adversarial attacks by viewing CycleGAN's training procedure as training a generator of adversarial examples and demonstrate that the cyclic consistency loss causes CycleGAN to be especially vulnerable to adversarial attacks.

NIPS 2017, workshop on Machine Deception},
 author = {Chu, Casey and Zhmoginov, Andrey and Sandler, Mark},
 date = {2017},
 title = {CycleGAN, a Master of Steganography},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1712.02950}
}


@article{Aggarwal.2021,
 author = {Aggarwal, Alankrita and Mittal, Mamta and Battineni, Gopi},
 year = {2021},
 title = {Generative adversarial network: An overview of theory and applications},
 pages = {100004},
 volume = {1},
 number = {1},
 issn = {26670968},
 journal = {International Journal of Information Management Data Insights},
 doi = {10.1016/j.jjimei.2020.100004}
}


@proceedings{.2019,
 year = {2019},
 title = {Proceedings of The 7th International Conference on Intelligent Systems and Image Processing 2019},
 publisher = {{The Institute of Industrial Application Engineers}},
 isbn = {9784907220198},
 doi = {10.12792/icisip2019}
}


@misc{Zhu.2017,
 abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping {\$}G: X $\backslash$rightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y $\backslash$rightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) $\backslash$approx X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.

An extended version of our ICCV 2017 paper, v7 fixed the typos and updated the implementation details. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
 author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
 date = {2017},
 title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1703.10593}
}


@inproceedings{Zhu.2019,
 author = {Zhu, Miao Miao and Gong, Shengrong and Qian, Zhenjiang and Zhang, Lifeng},
 title = {A Brief Review on Cycle Generative Adversarial Networks},
 pages = {235--242},
 publisher = {{The Institute of Industrial Application Engineers}},
 isbn = {9784907220198},
 booktitle = {Proceedings of The 7th International Conference on Intelligent Systems and Image Processing 2019},
 year = {2019},
 doi = {10.12792/icisip2019.046}
}


