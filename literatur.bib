% This file was created with Citavi 6.15.2.0

@book{.,
}


@proceedings{.2019,
 year = {2019},
 title = {Proceedings of The 7th International Conference on Intelligent Systems and Image Processing 2019},
 publisher = {{The Institute of Industrial Application Engineers}},
 isbn = {9784907220198},
 doi = {10.12792/icisip2019}
}


@article{Aggarwal.2021,
 author = {Aggarwal, Alankrita and Mittal, Mamta and Battineni, Gopi},
 year = {2021},
 title = {Generative adversarial network: An overview of theory and applications},
 pages = {100004},
 volume = {1},
 number = {1},
 issn = {26670968},
 journal = {International Journal of Information Management Data Insights},
 doi = {10.1016/j.jjimei.2020.100004}
}


@article{Creswell.2018,
 author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
 year = {2018},
 title = {Generative Adversarial Networks: An Overview},
 pages = {53--65},
 volume = {35},
 number = {1},
 issn = {1053-5888},
 journal = {IEEE Signal Processing Magazine},
 doi = {10.1109/MSP.2017.2765202}
}


@article{HazemAbdelmotaalAhmedA.AbdouAhmedF.OmarDaliaMohamedElSebaityKhaledAbdelazeem.2021,
 author = {{Hazem Abdelmotaal, Ahmed A. Abdou, Ahmed F. Omar, Dalia Mohamed El-Sebaity, Khaled Abdelazeem}},
 year = {2021},
 title = {Pix2pix Conditional Generative Adversarial Networks forScheimpflug Camera Color-Coded Corneal TomographyImage Generation}
}


@misc{He.2015,
 abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.

The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.

Tech report},
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 date = {2015},
 title = {Deep Residual Learning for Image Recognition},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1512.03385}
}


@incollection{PhillipIsola.,
 abstract = {1. Introduction

Many problems in image processing, computer graphics,},
 author = {{Phillip Isola} and {Jun-Yan Zhu} and {Tinghui Zhou} and {Alexei A. Efros}},
 title = {Image-to-Image Translation with Conditional Adversarial Networks}
}


@misc{Ulyanov.2016,
 abstract = {It this paper we revisit the fast stylization method introduced in Ulyanov et. al. (2016). We show how a small change in the stylization architecture results in a significant qualitative improvement in the generated images. The change is limited to swapping batch normalization with instance normalization, and to apply the latter both at training and testing times. The resulting method can be used to train high-performance architectures for real-time image generation. The code will is made available on github at https://github.com/DmitryUlyanov/texture{\_}nets. Full paper can be found at arXiv:1701.02096.},
 author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
 date = {2016},
 title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1607.08022}
}


@misc{Zhu.2017,
 abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain $X$ to a target domain $Y$ in the absence of paired examples. Our goal is to learn a mapping {\$}G: X $\backslash$rightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution $Y$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y $\backslash$rightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) $\backslash$approx X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.

An extended version of our ICCV 2017 paper, v7 fixed the typos and updated the implementation details. Code and data: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix},
 author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
 date = {2017},
 title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
 publisher = {arXiv},
 doi = {10.48550/arXiv.1703.10593}
}


@inproceedings{Zhu.2019,
 author = {Zhu, Miao Miao and Gong, Shengrong and Qian, Zhenjiang and Zhang, Lifeng},
 title = {A Brief Review on Cycle Generative Adversarial Networks},
 pages = {235--242},
 publisher = {{The Institute of Industrial Application Engineers}},
 isbn = {9784907220198},
 booktitle = {Proceedings of The 7th International Conference on Intelligent Systems and Image Processing 2019},
 year = {2019},
 doi = {10.12792/icisip2019.046}
}


@inproceedings{10.5555/3104322.3104425,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}